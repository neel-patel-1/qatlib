
QAT is a throughput oriented accelerator showing lower bytes/us processed at granularities lower than X when offloading synchronously from a single core to a single VF containing 4 PEs. 

When offloading asynchronously from a single core, we get higher bytes/us as requests are generated at a rate of X/s and load-balanced across 4 PEs.

Are VFs in a single PE isolated from one another?

Discussion:
Why have both a throughput oriented \dc \ax and low-latency \dc \ax on the same SoC?


arch:
banks: https://vscode.dev/github/neel-patel-1/qatlib/blob/qat_mwait/quickassist/lookaside/access_layer/src/qat_direct/common/include/adf_dev_ring_ctl.h#L50
rings: https://vscode.dev/github/neel-patel-1/qatlib/blob/qat_mwait/quickassist/lookaside/access_layer/src/qat_direct/common/include/adf_dev_ring_ctl.h#L69
accel-dev: https://vscode.dev/github/neel-patel-1/qatlib/blob/qat_mwait/quickassist/lookaside/access_layer/include/icp_accel_devices.h#L258

how many VFs? 16
how many banks per VF? 4 
how many rings per VF? 4 (https://cdrdv2-public.intel.com/743912/743912-qat-programmers-guide--rev03.pdf)
->
how many rings per bank? 1
do banks map to PEs? yes - hu-qtls (48 PEs on 1.7 devs -> 64 on 2.0 devs)
---

icp_adf_pollInstance is an alternative to ISR method -- user supplies trans_handle containing a ring number 
a transhandle has bank, accel, and ring: https://vscode.dev/github/neel-patel-1/qatlib/blob/qat_mwait/quickassist/lookaside/access_layer/src/qat_direct/common/adf_user_transport_ctrl.c#L529
callbacks registered for each trans_handle:https://vscode.dev/github/neel-patel-1/qatlib/blob/qat_mwait/quickassist/lookaside/access_layer/src/qat_direct/common/include/adf_dev_ring_ctl.h#L86

hu-qtls: The QAT hardware load-balances requests from all rings across all available computation engines.
iaa: enqueue request to dwq -> dq wd -> process
- effect of load-balancing requests across multiple processing engines
